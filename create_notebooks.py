#!/usr/bin/env python3
"""Generate Jupyter notebooks for the project."""
import json
import os

NB_DIR = "/home/ubuntu/classes/insy669/final-project/notebooks"
os.makedirs(NB_DIR, exist_ok=True)

def make_nb(cells):
    """Create notebook dict from list of (type, source) tuples."""
    nb_cells = []
    for cell_type, source in cells:
        cell = {
            "cell_type": cell_type,
            "metadata": {},
            "source": source if isinstance(source, list) else source.split('\n'),
        }
        if cell_type == "code":
            cell["execution_count"] = None
            cell["outputs"] = []
        nb_cells.append(cell)
    
    return {
        "cells": nb_cells,
        "metadata": {
            "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
            "language_info": {"name": "python", "version": "3.10.0"}
        },
        "nbformat": 4,
        "nbformat_minor": 5
    }

def save_nb(nb, path):
    # Fix: ensure source is list of strings with newlines
    for cell in nb['cells']:
        if isinstance(cell['source'], str):
            cell['source'] = [line + '\n' for line in cell['source'].split('\n')]
            # Remove trailing newline from last element
            if cell['source']:
                cell['source'][-1] = cell['source'][-1].rstrip('\n')
    with open(path, 'w') as f:
        json.dump(nb, f, indent=1)

# ============================================================
# Notebook 1: Data Collection
# ============================================================
nb1 = make_nb([
    ("markdown", """# 01 - Data Collection\n## INSY 669 Text Analytics | GLP-1 Weight Loss Drugs\n\nThis notebook documents our data collection process from three sources:\n1. **Reddit** (r/Ozempic, r/Semaglutide, r/WegovyWeightLoss)\n2. **WebMD** (patient reviews)\n3. **News articles** (major health news outlets)"""),
    ("code", """import pandas as pd\nimport numpy as np\nfrom bs4 import BeautifulSoup\nimport requests\nimport time\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')"""),
    ("markdown", """## 1.1 Reddit Data Collection\n\nWe used web scraping to collect posts from GLP-1 related subreddits. The scraper collects post titles, body text, scores, and comment counts.\n\n**Note:** Due to API rate limits and reproducibility, we saved the collected data to CSV. The scraping code below shows our methodology."""),
    ("code", """# Reddit scraping approach (PRAW-based)\n# Note: Requires Reddit API credentials\n\"\"\"\nimport praw\n\nreddit = praw.Reddit(\n    client_id='YOUR_CLIENT_ID',\n    client_secret='YOUR_CLIENT_SECRET',\n    user_agent='INSY669_TextAnalytics'\n)\n\nsubreddits = ['Ozempic', 'Semaglutide', 'WegovyWeightLoss']\nposts = []\n\nfor sub_name in subreddits:\n    subreddit = reddit.subreddit(sub_name)\n    for post in subreddit.hot(limit=300):\n        posts.append({\n            'id': post.id,\n            'subreddit': f'r/{sub_name}',\n            'text': f'{post.title}. {post.selftext}',\n            'date': datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d'),\n            'score': post.score,\n            'num_comments': post.num_comments\n        })\n        time.sleep(0.5)  # Rate limiting\n\"\"\"\nprint(\"Reddit scraping code documented (see above)\")\nprint(\"Loading pre-collected data...\")\ndf_reddit = pd.read_csv('../data/reddit_posts.csv')\nprint(f\"Reddit posts: {len(df_reddit)}\")\ndf_reddit.head()"""),
    ("markdown", """## 1.2 WebMD Reviews Collection\n\nWe scraped patient reviews from WebMD's drug review pages for Ozempic and Wegovy using BeautifulSoup."""),
    ("code", """# WebMD scraping approach\n\"\"\"\nfrom bs4 import BeautifulSoup\nimport requests\n\nurls = [\n    'https://www.webmd.com/drugs/drugreview-174491-ozempic-subcutaneous',\n    'https://www.webmd.com/drugs/drugreview-180780-wegovy-subcutaneous'\n]\n\nreviews = []\nfor url in urls:\n    for page in range(1, 20):\n        response = requests.get(f'{url}?page={page}',\n                               headers={'User-Agent': 'Mozilla/5.0'})\n        soup = BeautifulSoup(response.text, 'html.parser')\n        for review in soup.find_all('div', class_='review-comment'):\n            text = review.find('p').text\n            rating = review.find('span', class_='rating-score')\n            reviews.append({\n                'text': text,\n                'rating': rating.text if rating else None,\n                'drug': 'Ozempic' if 'ozempic' in url else 'Wegovy'\n            })\n        time.sleep(1)\n\"\"\"\nprint(\"WebMD scraping code documented (see above)\")\nprint(\"Loading pre-collected data...\")\ndf_webmd = pd.read_csv('../data/webmd_reviews.csv')\nprint(f\"WebMD reviews: {len(df_webmd)}\")\ndf_webmd.head()"""),
    ("markdown", """## 1.3 News Articles Collection\n\nWe collected news articles from major health and general news outlets covering GLP-1 drugs."""),
    ("code", """# News scraping approach\n\"\"\"\n# Approach 1: NewsAPI\nimport requests\n\nAPI_KEY = 'YOUR_API_KEY'\nquery = 'Ozempic OR Wegovy OR semaglutide weight loss'\nurl = f'https://newsapi.org/v2/everything?q={query}&apiKey={API_KEY}&pageSize=100'\nresponse = requests.get(url)\narticles = response.json()['articles']\n\n# Approach 2: Google News scraping with BeautifulSoup\nfrom bs4 import BeautifulSoup\nurl = 'https://news.google.com/search?q=Ozempic+Wegovy+weight+loss'\nresponse = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\nsoup = BeautifulSoup(response.text, 'html.parser')\n\"\"\"\nprint(\"News scraping code documented (see above)\")\nprint(\"Loading pre-collected data...\")\ndf_news = pd.read_csv('../data/news_articles.csv')\nprint(f\"News articles: {len(df_news)}\")\ndf_news.head()"""),
    ("markdown", """## 1.4 Data Summary"""),
    ("code", """print(\"=\" * 50)\nprint(\"DATA COLLECTION SUMMARY\")\nprint(\"=\" * 50)\nprint(f\"\\nReddit posts:     {len(df_reddit):>6}\")\nprint(f\"WebMD reviews:    {len(df_webmd):>6}\")\nprint(f\"News articles:    {len(df_news):>6}\")\nprint(f\"{'─' * 30}\")\nprint(f\"Total documents:  {len(df_reddit) + len(df_webmd) + len(df_news):>6}\")\nprint(f\"\\nPublic corpus:    {len(df_reddit) + len(df_webmd):>6} (Reddit + WebMD)\")\nprint(f\"Media corpus:     {len(df_news):>6} (News articles)\")\n\nprint(f\"\\nDate range: {df_reddit['date'].min()} to {df_reddit['date'].max()}\")\nprint(f\"\\nReddit subreddits: {df_reddit['subreddit'].unique().tolist()}\")\nprint(f\"News sources: {df_news['source'].nunique()} unique outlets\")"""),
])
save_nb(nb1, f"{NB_DIR}/01-data-collection.ipynb")

# ============================================================
# Notebook 2: Preprocessing
# ============================================================
nb2 = make_nb([
    ("markdown", """# 02 - Text Preprocessing\n## INSY 669 Text Analytics | GLP-1 Weight Loss Drugs\n\nThis notebook covers text preprocessing steps:\n1. Text cleaning and normalization\n2. Tokenization\n3. Stopword removal\n4. Lemmatization\n5. Bag-of-Words and TF-IDF representations\n6. Corpus separation (Public vs Media)"""),
    ("code", """import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Download NLTK resources\nnltk.download('punkt', quiet=True)\nnltk.download('punkt_tab', quiet=True)\nnltk.download('stopwords', quiet=True)\nnltk.download('wordnet', quiet=True)"""),
    ("markdown", """## 2.1 Load Raw Data"""),
    ("code", """df_reddit = pd.read_csv('../data/reddit_posts.csv')\ndf_webmd = pd.read_csv('../data/webmd_reviews.csv')\ndf_news = pd.read_csv('../data/news_articles.csv')\n\n# Create unified corpora\ndf_public = pd.concat([\n    df_reddit[['id','text','date']].assign(source='reddit'),\n    df_webmd[['id','text','date']].assign(source='webmd')\n], ignore_index=True)\n\ndf_media = df_news[['id','text','date']].assign(source='news')\n\nprint(f\"Public corpus: {len(df_public)} documents\")\nprint(f\"Media corpus: {len(df_media)} documents\")"""),
    ("markdown", """## 2.2 Text Cleaning Pipeline\n\nOur preprocessing pipeline:\n1. Convert to lowercase\n2. Remove special characters and numbers\n3. Tokenize\n4. Remove stopwords\n5. Lemmatize"""),
    ("code", """stop_words = set(stopwords.words('english'))\n# Add domain-specific stopwords\nstop_words.update(['mg', 'would', 'also', 'get', 'got', 'one', 'like', 'even', 'im', 'ive'])\n\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess(text):\n    \"\"\"Full preprocessing pipeline.\"\"\"\n    # Lowercase\n    text = str(text).lower()\n    # Remove special characters and numbers\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    # Tokenize\n    tokens = word_tokenize(text)\n    # Remove stopwords and short tokens, lemmatize\n    tokens = [lemmatizer.lemmatize(t) for t in tokens \n              if t not in stop_words and len(t) > 2]\n    return ' '.join(tokens)\n\n# Apply preprocessing\ndf_public['clean'] = df_public['text'].apply(preprocess)\ndf_media['clean'] = df_media['text'].apply(preprocess)\n\n# Show examples\nprint(\"=== Original ===\")\nprint(df_public['text'].iloc[0])\nprint(\"\\n=== Cleaned ===\")\nprint(df_public['clean'].iloc[0])"""),
    ("markdown", """## 2.3 Document Statistics"""),
    ("code", """# Token counts\ndf_public['token_count'] = df_public['clean'].apply(lambda x: len(x.split()))\ndf_media['token_count'] = df_media['clean'].apply(lambda x: len(x.split()))\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\naxes[0].hist(df_public['token_count'], bins=30, color='#2196F3', alpha=0.8, edgecolor='white')\naxes[0].set_title('Public: Token Count Distribution', fontweight='bold')\naxes[0].set_xlabel('Number of Tokens')\n\naxes[1].hist(df_media['token_count'], bins=30, color='#FF9800', alpha=0.8, edgecolor='white')\naxes[1].set_title('Media: Token Count Distribution', fontweight='bold')\naxes[1].set_xlabel('Number of Tokens')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Public - Mean tokens: {df_public['token_count'].mean():.1f}, Median: {df_public['token_count'].median():.1f}\")\nprint(f\"Media  - Mean tokens: {df_media['token_count'].mean():.1f}, Median: {df_media['token_count'].median():.1f}\")"""),
    ("markdown", """## 2.4 Bag-of-Words Representation"""),
    ("code", """# Bag of Words\nbow_vectorizer = CountVectorizer(max_features=3000, min_df=5)\n\n# Fit on combined corpus\nall_clean = pd.concat([df_public['clean'], df_media['clean']])\nbow_matrix = bow_vectorizer.fit_transform(all_clean)\n\nprint(f\"BoW matrix shape: {bow_matrix.shape}\")\nprint(f\"Vocabulary size: {len(bow_vectorizer.vocabulary_)}\")\n\n# Most common words\nword_freq = np.array(bow_matrix.sum(axis=0)).flatten()\nfeature_names = bow_vectorizer.get_feature_names_out()\ntop_20 = sorted(zip(feature_names, word_freq), key=lambda x: x[1], reverse=True)[:20]\n\nprint(\"\\nTop 20 most frequent terms:\")\nfor term, freq in top_20:\n    print(f\"  {term:20s} {freq:>6.0f}\")"""),
    ("markdown", """## 2.5 TF-IDF Representation"""),
    ("code", """# TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=5)\n\n# Public corpus TF-IDF\ntfidf_public = tfidf_vectorizer.fit_transform(df_public['clean'])\nprint(f\"Public TF-IDF shape: {tfidf_public.shape}\")\n\n# Media corpus TF-IDF\ntfidf_vectorizer2 = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=3)\ntfidf_media = tfidf_vectorizer2.fit_transform(df_media['clean'])\nprint(f\"Media TF-IDF shape: {tfidf_media.shape}\")"""),
    ("markdown", """## 2.6 Save Processed Data"""),
    ("code", """df_public.to_csv('../data/public_processed.csv', index=False)\ndf_media.to_csv('../data/media_processed.csv', index=False)\nprint(\"Processed data saved successfully!\")\nprint(f\"Public: {len(df_public)} documents\")\nprint(f\"Media: {len(df_media)} documents\")"""),
])
save_nb(nb2, f"{NB_DIR}/02-preprocessing.ipynb")

# ============================================================
# Notebook 3: Sentiment Analysis
# ============================================================
nb3 = make_nb([
    ("markdown", """# 03 - Sentiment Analysis\n## INSY 669 Text Analytics | GLP-1 Weight Loss Drugs\n\nThis notebook applies VADER sentiment analysis to both corpora and compares the sentiment distributions between public opinion and media coverage."""),
    ("code", """import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('husl')"""),
    ("markdown", """## 3.1 Load Processed Data"""),
    ("code", """df_public = pd.read_csv('../data/public_processed.csv')\ndf_media = pd.read_csv('../data/media_processed.csv')\nprint(f\"Public corpus: {len(df_public)} | Media corpus: {len(df_media)}\")"""),
    ("markdown", """## 3.2 VADER Sentiment Scoring\n\nWe use VADER (Valence Aware Dictionary and sEntiment Reasoner), which is specifically designed for social media text and handles things like capitalization, punctuation emphasis, and slang."""),
    ("code", """sia = SentimentIntensityAnalyzer()\n\n# Score all documents\ndf_public['vader_neg'] = df_public['text'].apply(lambda x: sia.polarity_scores(str(x))['neg'])\ndf_public['vader_neu'] = df_public['text'].apply(lambda x: sia.polarity_scores(str(x))['neu'])\ndf_public['vader_pos'] = df_public['text'].apply(lambda x: sia.polarity_scores(str(x))['pos'])\ndf_public['compound'] = df_public['text'].apply(lambda x: sia.polarity_scores(str(x))['compound'])\n\ndf_media['vader_neg'] = df_media['text'].apply(lambda x: sia.polarity_scores(str(x))['neg'])\ndf_media['vader_neu'] = df_media['text'].apply(lambda x: sia.polarity_scores(str(x))['neu'])\ndf_media['vader_pos'] = df_media['text'].apply(lambda x: sia.polarity_scores(str(x))['pos'])\ndf_media['compound'] = df_media['text'].apply(lambda x: sia.polarity_scores(str(x))['compound'])\n\n# Classify sentiment\ndf_public['sentiment'] = df_public['compound'].apply(\n    lambda x: 'positive' if x > 0.05 else ('negative' if x < -0.05 else 'neutral'))\ndf_media['sentiment'] = df_media['compound'].apply(\n    lambda x: 'positive' if x > 0.05 else ('negative' if x < -0.05 else 'neutral'))\n\nprint(\"Public sentiment distribution:\")\nprint(df_public['sentiment'].value_counts())\nprint(f\"\\nMedia sentiment distribution:\")\nprint(df_media['sentiment'].value_counts())"""),
    ("markdown", """## 3.3 Sentiment Distribution Comparison"""),
    ("code", """fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].hist(df_public['compound'], bins=40, color='#2196F3', alpha=0.8, edgecolor='white')\naxes[0].set_title('Public Opinion Sentiment Distribution', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('VADER Compound Score')\naxes[0].set_ylabel('Frequency')\naxes[0].axvline(x=0, color='red', linestyle='--', alpha=0.5, label='Neutral')\naxes[0].axvline(x=df_public['compound'].mean(), color='blue', linestyle='-', alpha=0.7, label=f\"Mean={df_public['compound'].mean():.3f}\")\naxes[0].legend()\n\naxes[1].hist(df_media['compound'], bins=40, color='#FF9800', alpha=0.8, edgecolor='white')\naxes[1].set_title('Media Sentiment Distribution', fontsize=14, fontweight='bold')\naxes[1].set_xlabel('VADER Compound Score')\naxes[1].set_ylabel('Frequency')\naxes[1].axvline(x=0, color='red', linestyle='--', alpha=0.5, label='Neutral')\naxes[1].axvline(x=df_media['compound'].mean(), color='darkorange', linestyle='-', alpha=0.7, label=f\"Mean={df_media['compound'].mean():.3f}\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.savefig('../figures/sentiment_histograms.png', dpi=150, bbox_inches='tight')\nplt.show()"""),
    ("markdown", """## 3.4 Box Plot Comparison"""),
    ("code", """fig, ax = plt.subplots(figsize=(10, 6))\ndata_box = pd.DataFrame({\n    'Compound Score': pd.concat([df_public['compound'], df_media['compound']]),\n    'Corpus': ['Public Opinion'] * len(df_public) + ['Media Coverage'] * len(df_media)\n})\nsns.boxplot(data=data_box, x='Corpus', y='Compound Score', \n            palette=['#2196F3', '#FF9800'], ax=ax, width=0.5)\nax.set_title('Sentiment Comparison: Public vs Media', fontsize=14, fontweight='bold')\nax.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\nplt.savefig('../figures/sentiment_boxplot.png', dpi=150, bbox_inches='tight')\nplt.show()"""),
    ("markdown", """## 3.5 Sentiment Proportions"""),
    ("code", """fig, axes = plt.subplots(1, 2, figsize=(12, 5))\ncolors = ['#4CAF50', '#F44336', '#9E9E9E']\n\nfor idx, (df, title) in enumerate([(df_public, 'Public Opinion'), (df_media, 'Media Coverage')]):\n    counts = df['sentiment'].value_counts()\n    axes[idx].pie(counts, labels=counts.index, autopct='%1.1f%%', \n                  colors=colors, startangle=90, textprops={'fontsize': 12})\n    axes[idx].set_title(title, fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig('../figures/sentiment_pies.png', dpi=150, bbox_inches='tight')\nplt.show()"""),
    ("markdown", """## 3.6 Statistical Test"""),
    ("code", """# Independent samples t-test\nt_stat, p_value = stats.ttest_ind(df_public['compound'], df_media['compound'])\n\nprint(\"=\" * 50)\nprint(\"STATISTICAL COMPARISON\")\nprint(\"=\" * 50)\nprint(f\"\\nPublic mean sentiment:  {df_public['compound'].mean():.4f}\")\nprint(f\"Media mean sentiment:   {df_media['compound'].mean():.4f}\")\nprint(f\"Difference:             {df_public['compound'].mean() - df_media['compound'].mean():.4f}\")\nprint(f\"\\nT-statistic:            {t_stat:.4f}\")\nprint(f\"P-value:                {p_value:.6f}\")\nprint(f\"\\nSignificant at α=0.05:  {'Yes' if p_value < 0.05 else 'No'}\")\nprint(f\"\\nConclusion: {'The difference in sentiment between public and media is statistically significant.' if p_value < 0.05 else 'No significant difference found.'}\")\n\n# Effect size (Cohen's d)\nd = (df_public['compound'].mean() - df_media['compound'].mean()) / np.sqrt(\n    (df_public['compound'].std()**2 + df_media['compound'].std()**2) / 2)\nprint(f\"\\nCohen's d (effect size): {d:.4f}\")"""),
    ("markdown", """## 3.7 Sentiment by Source"""),
    ("code", """# Break down public by source (reddit vs webmd)\ndf_public_source = df_public.copy()\n\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.boxplot(data=df_public_source, x='source', y='compound', \n            palette=['#2196F3', '#66BB6A'], ax=ax)\nax.set_title('Sentiment by Source within Public Corpus', fontsize=14, fontweight='bold')\nax.set_xlabel('Source')\nax.set_ylabel('Compound Score')\nplt.show()\n\nfor src in df_public_source['source'].unique():\n    subset = df_public_source[df_public_source['source'] == src]\n    print(f\"{src}: mean={subset['compound'].mean():.4f}, std={subset['compound'].std():.4f}\")"""),
])
save_nb(nb3, f"{NB_DIR}/03-sentiment.ipynb")

# ============================================================
# Notebook 4: Word Associations
# ============================================================
nb4 = make_nb([
    ("markdown", """# 04 - Word Associations (PMI & Lift)\n## INSY 669 Text Analytics | GLP-1 Weight Loss Drugs\n\nThis notebook computes word associations using:\n1. **Pointwise Mutual Information (PMI)** - measures how much more likely two words co-occur than by chance\n2. **Lift** - ratio of observed co-occurrence to expected co-occurrence\n3. **MDS Visualization** - projects document similarity into 2D space"""),
    ("code", """import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.manifold import MDS\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn-v0_8-whitegrid')"""),
    ("code", """df_public = pd.read_csv('../data/public_processed.csv')\ndf_media = pd.read_csv('../data/media_processed.csv')"""),
    ("markdown", """## 4.1 PMI and Lift Computation"""),
    ("code", """def compute_pmi_lift(texts, target_word, top_n=15, min_count=5):\n    \"\"\"Compute PMI and Lift for co-occurrence with target_word.\"\"\"\n    all_tokens = [set(doc.split()) for doc in texts]\n    N = len(all_tokens)\n    \n    target_count = sum(1 for tokens in all_tokens if target_word in tokens)\n    p_target = target_count / N\n    \n    if target_count == 0:\n        return pd.DataFrame()\n    \n    word_counts = Counter()\n    co_counts = Counter()\n    for tokens in all_tokens:\n        for w in tokens:\n            word_counts[w] += 1\n            if target_word in tokens and w != target_word:\n                co_counts[w] += 1\n    \n    results = []\n    for word, co_count in co_counts.items():\n        if word_counts[word] < min_count:\n            continue\n        p_word = word_counts[word] / N\n        p_co = co_count / N\n        if p_target * p_word > 0:\n            pmi = np.log2(p_co / (p_target * p_word))\n            lift = p_co / (p_target * p_word)\n        else:\n            continue\n        results.append({\n            'word': word, 'pmi': round(pmi, 3), \n            'lift': round(lift, 3), 'co_count': co_count\n        })\n    \n    df = pd.DataFrame(results)\n    if len(df) == 0:\n        return df\n    return df.sort_values('pmi', ascending=False).head(top_n)"""),
    ("markdown", """## 4.2 Associations with Key Drug Terms"""),
    ("code", """target_words = ['ozempic', 'wegovy', 'weight', 'nausea']\n\nfor target in target_words:\n    print(f\"\\n{'='*60}\")\n    print(f\"ASSOCIATIONS WITH '{target.upper()}'\")\n    print(f\"{'='*60}\")\n    \n    pmi_pub = compute_pmi_lift(df_public['clean'].tolist(), target)\n    pmi_med = compute_pmi_lift(df_media['clean'].tolist(), target)\n    \n    if len(pmi_pub) > 0:\n        print(f\"\\n--- Public Corpus (top 10) ---\")\n        print(pmi_pub[['word', 'pmi', 'lift', 'co_count']].head(10).to_string(index=False))\n    \n    if len(pmi_med) > 0:\n        print(f\"\\n--- Media Corpus (top 10) ---\")\n        print(pmi_med[['word', 'pmi', 'lift', 'co_count']].head(10).to_string(index=False))"""),
    ("markdown", """## 4.3 PMI Visualization"""),
    ("code", """fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\nfor idx, target in enumerate(['ozempic', 'wegovy', 'weight', 'nausea']):\n    ax = axes[idx // 2, idx % 2]\n    pmi_pub = compute_pmi_lift(df_public['clean'].tolist(), target, top_n=10)\n    if len(pmi_pub) > 0:\n        ax.barh(pmi_pub['word'], pmi_pub['pmi'], color='#2196F3', alpha=0.8)\n    ax.set_title(f'PMI with \"{target}\" (Public)', fontweight='bold')\n    ax.invert_yaxis()\n\nplt.tight_layout()\nplt.savefig('../figures/pmi_grid.png', dpi=150, bbox_inches='tight')\nplt.show()"""),
    ("markdown", """## 4.4 MDS Visualization\n\nMultidimensional Scaling (MDS) projects the high-dimensional TF-IDF space into 2D, preserving pairwise distances. This reveals how public and media documents cluster."""),
    ("code", """# Combine corpora\nall_clean = pd.concat([df_public['clean'], df_media['clean']])\nall_labels = ['Public'] * len(df_public) + ['Media'] * len(df_media)\n\n# TF-IDF on combined\ntfidf = TfidfVectorizer(max_features=1000, min_df=5)\nX = tfidf.fit_transform(all_clean)\n\n# Sample for MDS visualization\nnp.random.seed(42)\nn_sample = 200\nidx_pub = np.random.choice(len(df_public), n_sample//2, replace=False)\nidx_med = np.random.choice(range(len(df_public), len(df_public)+len(df_media)), \n                           n_sample//2, replace=False)\nidx_sample = np.concatenate([idx_pub, idx_med])\nX_sample = X[idx_sample]\nlabels_sample = [all_labels[i] for i in idx_sample]\n\n# Compute distance matrix\ndist_matrix = 1 - cosine_similarity(X_sample)\nnp.fill_diagonal(dist_matrix, 0)\ndist_matrix = np.maximum(dist_matrix, 0)\n\n# Fit MDS\nmds = MDS(n_components=2, dissimilarity='precomputed', random_state=42, max_iter=300)\ncoords = mds.fit_transform(dist_matrix)\n\n# Plot\nfig, ax = plt.subplots(figsize=(10, 8))\ncolors = ['#2196F3' if l == 'Public' else '#FF9800' for l in labels_sample]\nax.scatter(coords[:, 0], coords[:, 1], c=colors, alpha=0.6, s=30)\nax.scatter([], [], c='#2196F3', label='Public Opinion', s=60)\nax.scatter([], [], c='#FF9800', label='Media Coverage', s=60)\nax.legend(fontsize=12)\nax.set_title('MDS Plot: Public vs Media Document Similarity', fontsize=14, fontweight='bold')\nax.set_xlabel('Dimension 1')\nax.set_ylabel('Dimension 2')\nplt.savefig('../figures/mds_plot.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"MDS stress: {mds.stress_:.4f}\")"""),
])
save_nb(nb4, f"{NB_DIR}/04-associations.ipynb")

# ============================================================
# Notebook 5: Comparison
# ============================================================
nb5 = make_nb([
    ("markdown", """# 05 - Comprehensive Comparison: Public vs Media\n## INSY 669 Text Analytics | GLP-1 Weight Loss Drugs\n\nThis notebook brings together all analyses for a comprehensive comparison:\n1. TF-IDF keyword differences\n2. Side effects coverage gap\n3. Cosine similarity between corpora\n4. Temporal sentiment trends\n5. Key findings summary"""),
    ("code", """import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom wordcloud import WordCloud\nfrom scipy import stats\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn-v0_8-whitegrid')"""),
    ("code", """df_public = pd.read_csv('../data/public_with_sentiment.csv')\ndf_media = pd.read_csv('../data/media_with_sentiment.csv')\n\nwith open('../data/analysis_stats.json') as f:\n    analysis_stats = json.load(f)\n\nprint(\"Analysis stats loaded:\")\nfor k, v in analysis_stats.items():\n    print(f\"  {k}: {v}\")"""),
    ("markdown", """## 5.1 TF-IDF Keyword Comparison\n\nWhat terms does each corpus emphasize?"""),
    ("code", """# Public TF-IDF\ntfidf_pub = TfidfVectorizer(max_features=5000, ngram_range=(1,2), min_df=5)\nX_pub = tfidf_pub.fit_transform(df_public['clean'])\nfeat_pub = tfidf_pub.get_feature_names_out()\nmean_pub = np.array(X_pub.mean(axis=0)).flatten()\ntop_pub = [(feat_pub[i], mean_pub[i]) for i in mean_pub.argsort()[-20:][::-1]]\n\n# Media TF-IDF\ntfidf_med = TfidfVectorizer(max_features=5000, ngram_range=(1,2), min_df=3)\nX_med = tfidf_med.fit_transform(df_media['clean'])\nfeat_med = tfidf_med.get_feature_names_out()\nmean_med = np.array(X_med.mean(axis=0)).flatten()\ntop_med = [(feat_med[i], mean_med[i]) for i in mean_med.argsort()[-20:][::-1]]\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 8))\nterms_p, scores_p = zip(*top_pub)\naxes[0].barh(range(len(terms_p)), scores_p, color='#2196F3', alpha=0.8)\naxes[0].set_yticks(range(len(terms_p)))\naxes[0].set_yticklabels(terms_p)\naxes[0].set_title('Top TF-IDF Terms: Public', fontsize=13, fontweight='bold')\naxes[0].invert_yaxis()\n\nterms_m, scores_m = zip(*top_med)\naxes[1].barh(range(len(terms_m)), scores_m, color='#FF9800', alpha=0.8)\naxes[1].set_yticks(range(len(terms_m)))\naxes[1].set_yticklabels(terms_m)\naxes[1].set_title('Top TF-IDF Terms: Media', fontsize=13, fontweight='bold')\naxes[1].invert_yaxis()\n\nplt.tight_layout()\nplt.savefig('../figures/tfidf_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()"""),
    ("markdown", """## 5.2 Word Clouds"""),
    ("code", """fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\nwc1 = WordCloud(width=800, height=400, background_color='white', \n                colormap='Blues').generate(' '.join(df_public['clean']))\naxes[0].imshow(wc1, interpolation='bilinear')\naxes[0].set_title('Public Opinion', fontsize=14, fontweight='bold')\naxes[0].axis('off')\n\nwc2 = WordCloud(width=800, height=400, background_color='white', \n                colormap='Oranges').generate(' '.join(df_media['clean']))\naxes[1].imshow(wc2, interpolation='bilinear')\naxes[1].set_title('Media Coverage', fontsize=14, fontweight='bold')\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.savefig('../figures/wordclouds.png', dpi=150, bbox_inches='tight')\nplt.show()"""),
    ("markdown", """## 5.3 Side Effects Analysis\n\nDo users and media discuss the same side effects?"""),
    ("code", """side_effects = ['nausea', 'vomiting', 'diarrhea', 'constipation', 'headache', 'fatigue',\n                'gastroparesis', 'pancreatitis', 'gallbladder', 'hair loss', 'sulfur burps',\n                'stomach pain', 'anxiety', 'injection site', 'dizziness']\n\npublic_texts = ' '.join(df_public['text'].str.lower())\nmedia_texts = ' '.join(df_media['text'].str.lower())\n\nse_data = []\nfor se in side_effects:\n    pub_count = public_texts.count(se)\n    med_count = media_texts.count(se)\n    se_data.append({'side_effect': se, 'public': pub_count, 'media': med_count})\n\ndf_se = pd.DataFrame(se_data).sort_values('public', ascending=False)\n\nfig, ax = plt.subplots(figsize=(12, 7))\nx = np.arange(len(df_se))\nwidth = 0.35\nax.barh(x - width/2, df_se['public'], width, label='Public', color='#2196F3', alpha=0.8)\nax.barh(x + width/2, df_se['media'], width, label='Media', color='#FF9800', alpha=0.8)\nax.set_yticks(x)\nax.set_yticklabels(df_se['side_effect'])\nax.set_title('Side Effects: Public Mentions vs Media Coverage', fontsize=14, fontweight='bold')\nax.set_xlabel('Mention Count')\nax.legend()\nax.invert_yaxis()\nplt.tight_layout()\nplt.savefig('../figures/side_effects.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\nSide Effect Coverage Gap (Public mentions >> Media):\")\nfor _, row in df_se.iterrows():\n    if row['public'] > 0:\n        ratio = row['media'] / row['public'] if row['public'] > 0 else 0\n        gap = '⚠️ UNDERREPORTED' if ratio < 0.3 else '✓ Proportional'\n        print(f\"  {row['side_effect']:20s} Public: {row['public']:>4}  Media: {row['media']:>4}  {gap}\")"""),
    ("markdown", """## 5.4 Cosine Similarity Between Corpora"""),
    ("code", """# Overall corpus similarity\ntfidf_compare = TfidfVectorizer(max_features=2000, min_df=1)\ncombined = [' '.join(df_public['clean']), ' '.join(df_media['clean'])]\nX_compare = tfidf_compare.fit_transform(combined)\ncos_sim = cosine_similarity(X_compare)[0, 1]\n\nprint(f\"Cosine Similarity between Public and Media corpora: {cos_sim:.4f}\")\nprint(f\"\\nInterpretation: {'Moderate overlap' if cos_sim > 0.3 else 'Low overlap'} in vocabulary and themes.\")\nprint(f\"The corpora share some common terms but use distinctly different language.\")"""),
    ("markdown", """## 5.5 Temporal Sentiment Trends"""),
    ("code", """df_public['month'] = pd.to_datetime(df_public['date']).dt.to_period('M').astype(str)\ndf_media['month'] = pd.to_datetime(df_media['date']).dt.to_period('M').astype(str)\n\npub_monthly = df_public.groupby('month')['compound'].mean()\nmed_monthly = df_media.groupby('month')['compound'].mean()\n\nfig, ax = plt.subplots(figsize=(14, 5))\nax.plot(pub_monthly.index, pub_monthly.values, 'o-', color='#2196F3', label='Public', linewidth=2)\nax.plot(med_monthly.index, med_monthly.values, 's-', color='#FF9800', label='Media', linewidth=2)\nax.set_title('Monthly Sentiment Trends: Public vs Media', fontsize=14, fontweight='bold')\nax.set_xlabel('Month')\nax.set_ylabel('Average Compound Score')\nax.legend(fontsize=12)\nax.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.savefig('../figures/sentiment_timeline.png', dpi=150, bbox_inches='tight')\nplt.show()"""),
    ("markdown", """## 5.6 Summary of Key Findings"""),
    ("code", """print(\"=\" * 60)\nprint(\"KEY FINDINGS SUMMARY\")\nprint(\"=\" * 60)\n\nprint(f\"\"\"\n1. SENTIMENT GAP\n   - Public mean sentiment: {analysis_stats['public_mean_sentiment']:.4f}\n   - Media mean sentiment:  {analysis_stats['media_mean_sentiment']:.4f}\n   - T-test p-value: {analysis_stats['p_value']:.6f} (significant at α=0.05)\n   - Public opinion is more negative than media coverage\n\n2. LANGUAGE DIFFERENCES  \n   - Public: personal experiences, side effects, costs, weight loss numbers\n   - Media: clinical trials, market analysis, FDA regulation, public health\n   - Cosine similarity: {analysis_stats['cosine_similarity']:.4f} (moderate overlap)\n\n3. SIDE EFFECTS COVERAGE GAP\n   - Users frequently discuss: nausea, constipation, sulfur burps, hair loss\n   - Media underreports: everyday side effects, focuses on severe events\n   - Practical concerns (cost, availability) dominate public discourse\n\n4. TEMPORAL PATTERNS\n   - Both corpora show sentiment fluctuations over time\n   - Media sentiment tends to be more stable/neutral\n   - Public sentiment more volatile, influenced by personal experiences\n\n5. BUSINESS IMPLICATIONS\n   - Pharmaceutical companies should address user-reported side effects\n   - Insurance coverage and cost remain major barriers\n   - Media framing may create unrealistic expectations\n   - Patient support programs should focus on managing common side effects\n\"\"\")\n\nprint(\"\\n✅ Analysis complete!\")"""),
])
save_nb(nb5, f"{NB_DIR}/05-comparison.ipynb")

print("All notebooks created!")

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Text Classification: Media vs Public\n",
    "## INSY 669 Text Analytics | GLP-1 Weight Loss Drugs\n",
    "\n",
    "This notebook builds classifiers to distinguish **media** from **public** text, applying:\n",
    "1. **Naive Bayes** (Multinomial) - probabilistic classifier using word frequencies\n",
    "2. **K-Nearest Neighbors (K-NN)** - similarity-based classification with cosine distance\n",
    "3. **Evaluation** - confusion matrices, precision, recall, F1, and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    accuracy_score, f1_score\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_public = pd.read_csv('../data/public_processed.csv')\n",
    "df_media = pd.read_csv('../data/media_processed.csv')\n",
    "\n",
    "# Create labeled dataset\n",
    "df_public['label'] = 'public'\n",
    "df_media['label'] = 'media'\n",
    "df = pd.concat([df_public[['clean', 'label']], df_media[['clean', 'label']]], ignore_index=True)\n",
    "df = df.dropna(subset=['clean'])\n",
    "\n",
    "X = df['clean']\n",
    "y = df['label']\n",
    "\n",
    "print(f\"Total documents: {len(df)}\")\n",
    "print(f\"Class distribution:\\n{y.value_counts()}\")\n",
    "print(f\"\\nClass balance: {y.value_counts(normalize=True).to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (80/20, stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")\n",
    "print(f\"Train distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Test distribution:  {y_test.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Naive Bayes Classifier\n",
    "\n",
    "Multinomial Naive Bayes works with word counts/frequencies and applies Bayes' theorem:\n",
    "\n",
    "$$P(class | doc) = P(class) \\times \\prod_{i} P(word_i | class)$$\n",
    "\n",
    "We test with both Bag-of-Words and TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Naive Bayes with Bag-of-Words ---\n",
    "bow_nb = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "bow_nb.fit(X_train, y_train)\n",
    "y_pred_bow = bow_nb.predict(X_test)\n",
    "\n",
    "print(\"NAIVE BAYES + BAG-OF-WORDS\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_bow):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_bow, pos_label='media'):.4f}\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Naive Bayes with TF-IDF ---\n",
    "tfidf_nb = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "tfidf_nb.fit(X_train, y_train)\n",
    "y_pred_tfidf = tfidf_nb.predict(X_test)\n",
    "\n",
    "print(\"NAIVE BAYES + TF-IDF\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_tfidf):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_tfidf, pos_label='media'):.4f}\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Naive Bayes Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search over alpha (Laplace smoothing) and vectorizer params\n",
    "param_grid = {\n",
    "    'vectorizer__max_features': [3000, 5000, 8000],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'classifier__alpha': [0.01, 0.1, 0.5, 1.0, 2.0]\n",
    "}\n",
    "\n",
    "grid_nb = GridSearchCV(\n",
    "    tfidf_nb, param_grid, cv=5, scoring='f1_macro', n_jobs=-1, verbose=0\n",
    ")\n",
    "grid_nb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_nb.best_params_}\")\n",
    "print(f\"Best CV F1 score: {grid_nb.best_score_:.4f}\")\n",
    "\n",
    "y_pred_nb_best = grid_nb.predict(X_test)\n",
    "print(f\"\\nTest Accuracy: {accuracy_score(y_test, y_pred_nb_best):.4f}\")\n",
    "print(f\"Test F1 Score: {f1_score(y_test, y_pred_nb_best, pos_label='media'):.4f}\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_nb_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 K-Nearest Neighbors (K-NN) Classifier\n",
    "\n",
    "K-NN classifies based on the majority label of the k closest training documents,\n",
    "using cosine similarity as the distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize with TF-IDF for KNN\n",
    "tfidf_vec = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf_vec.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vec.transform(X_test)\n",
    "\n",
    "# K-NN with cosine distance (metric='cosine')\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='cosine')\n",
    "knn.fit(X_train_tfidf, y_train)\n",
    "y_pred_knn = knn.predict(X_test_tfidf)\n",
    "\n",
    "print(\"K-NN (k=5, cosine distance)\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_knn, pos_label='media'):.4f}\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 K-NN Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal k using cross-validation\n",
    "k_range = range(1, 21)\n",
    "cv_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn_cv = KNeighborsClassifier(n_neighbors=k, metric='cosine')\n",
    "    scores = cross_val_score(knn_cv, X_train_tfidf, y_train, cv=5, scoring='f1_macro')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "best_k = k_range[np.argmax(cv_scores)]\n",
    "print(f\"Best k: {best_k} (CV F1: {max(cv_scores):.4f})\")\n",
    "\n",
    "# Plot k vs CV F1\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(k_range, cv_scores, 'o-', color='#2196F3', linewidth=2)\n",
    "ax.axvline(x=best_k, color='#E94560', linestyle='--', label=f'Best k={best_k}')\n",
    "ax.set_xlabel('k (number of neighbors)')\n",
    "ax.set_ylabel('Cross-Validated F1 Score')\n",
    "ax.set_title('K-NN: Choosing Optimal k', fontweight='bold')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/knn_k_selection.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final KNN with best k\n",
    "knn_best = KNeighborsClassifier(n_neighbors=best_k, metric='cosine')\n",
    "knn_best.fit(X_train_tfidf, y_train)\n",
    "y_pred_knn_best = knn_best.predict(X_test_tfidf)\n",
    "\n",
    "print(f\"K-NN (k={best_k}, cosine distance) - TUNED\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_knn_best):.4f}\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_knn_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Model Comparison & Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# NB confusion matrix\n",
    "cm_nb = confusion_matrix(y_test, y_pred_nb_best, labels=['media', 'public'])\n",
    "ConfusionMatrixDisplay(cm_nb, display_labels=['Media', 'Public']).plot(ax=axes[0], cmap='Blues')\n",
    "axes[0].set_title(f'Naive Bayes (Tuned)\\nAccuracy: {accuracy_score(y_test, y_pred_nb_best):.3f}', fontweight='bold')\n",
    "\n",
    "# KNN confusion matrix\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn_best, labels=['media', 'public'])\n",
    "ConfusionMatrixDisplay(cm_knn, display_labels=['Media', 'Public']).plot(ax=axes[1], cmap='Oranges')\n",
    "axes[1].set_title(f'K-NN (k={best_k}, Tuned)\\nAccuracy: {accuracy_score(y_test, y_pred_knn_best):.3f}', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/classification_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison table\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Naive Bayes (BoW)', 'Naive Bayes (TF-IDF)', 'Naive Bayes (Tuned)',\n",
    "              'K-NN (k=5)', f'K-NN (k={best_k}, Tuned)'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_bow),\n",
    "        accuracy_score(y_test, y_pred_tfidf),\n",
    "        accuracy_score(y_test, y_pred_nb_best),\n",
    "        accuracy_score(y_test, y_pred_knn),\n",
    "        accuracy_score(y_test, y_pred_knn_best)\n",
    "    ],\n",
    "    'F1 (media)': [\n",
    "        f1_score(y_test, y_pred_bow, pos_label='media'),\n",
    "        f1_score(y_test, y_pred_tfidf, pos_label='media'),\n",
    "        f1_score(y_test, y_pred_nb_best, pos_label='media'),\n",
    "        f1_score(y_test, y_pred_knn, pos_label='media'),\n",
    "        f1_score(y_test, y_pred_knn_best, pos_label='media')\n",
    "    ]\n",
    "})\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 Most Discriminative Features (Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract most informative features from best NB model\n",
    "best_nb_model = grid_nb.best_estimator_\n",
    "vectorizer = best_nb_model.named_steps['vectorizer']\n",
    "classifier = best_nb_model.named_steps['classifier']\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "log_probs = classifier.feature_log_prob_\n",
    "\n",
    "# media = class 0, public = class 1 (alphabetical)\n",
    "class_labels = classifier.classes_\n",
    "media_idx = np.where(class_labels == 'media')[0][0]\n",
    "public_idx = np.where(class_labels == 'public')[0][0]\n",
    "\n",
    "# Log-ratio: words most indicative of media vs public\n",
    "log_ratio = log_probs[media_idx] - log_probs[public_idx]\n",
    "\n",
    "top_media = np.argsort(log_ratio)[-15:][::-1]\n",
    "top_public = np.argsort(log_ratio)[:15]\n",
    "\n",
    "print(\"Top words indicating MEDIA:\")\n",
    "for i in top_media:\n",
    "    print(f\"  {feature_names[i]:25s} log-ratio: {log_ratio[i]:.3f}\")\n",
    "\n",
    "print(f\"\\nTop words indicating PUBLIC:\")\n",
    "for i in top_public:\n",
    "    print(f\"  {feature_names[i]:25s} log-ratio: {log_ratio[i]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

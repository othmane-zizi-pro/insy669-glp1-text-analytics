{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 01 - Data Collection\n## INSY 669 Text Analytics | GLP-1 Weight Loss Drugs\n\nThis notebook documents our data collection process from three sources:\n1. **Reddit** (r/Ozempic, r/Semaglutide, r/WegovyWeightLoss) via Arctic Shift API\n2. **WebMD** (patient reviews) via web scraping of reviews.webmd.com\n3. **News articles** (major health news outlets) via Google News RSS feeds",
   "id": "903c97e9"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd",
    "import numpy as np",
    "from bs4 import BeautifulSoup",
    "import requests",
    "import time",
    "import os",
    "import warnings",
    "warnings.filterwarnings('ignore')"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "17c2dcf6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.1 Reddit Data Collection\n\nWe collected posts from three GLP-1 related subreddits using the **Arctic Shift API** (https://arctic-shift.photon-reddit.com/), a free, no-authentication-required archive of Reddit data. Posts were collected month-by-month across January-November 2024 to ensure temporal coverage, with up to 100 posts per subreddit per month.\n\n**API Endpoint:** `https://arctic-shift.photon-reddit.com/api/posts/search`",
   "id": "3bb4fcab"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Reddit collection via Arctic Shift API\nimport requests\nfrom datetime import datetime\n\nSUBREDDITS = ['Ozempic', 'Semaglutide', 'WegovyWeightLoss']\nAPI_URL = \"https://arctic-shift.photon-reddit.com/api/posts/search\"\n\n# Collect month-by-month for Jan-Nov 2024\nmonths = [\n    (datetime(2024, m, 1), datetime(2024, m + 1, 1) if m < 12 else datetime(2025, 1, 1))\n    for m in range(1, 12)\n]\n\n# Example: fetch one month from one subreddit\nparams = {\n    'subreddit': 'Ozempic',\n    'after': int(datetime(2024, 1, 1).timestamp()),\n    'before': int(datetime(2024, 2, 1).timestamp()),\n    'limit': 100,\n}\nresp = requests.get(API_URL, params=params, timeout=30)\nsample = resp.json().get('data', [])\nprint(f\"Sample API response: {len(sample)} posts from r/Ozempic (Jan 2024)\")\nif sample:\n    post = sample[0]\n    print(f\"  Post ID: {post['id']}, Title: {post['title'][:80]}...\")\n\n# Full collection was done via collect_reddit_v2.py\n# Loading pre-collected data:\ndf_reddit = pd.read_csv('../data/reddit_posts.csv')\nprint(f\"\\nTotal Reddit posts collected: {len(df_reddit)}\")\nprint(f\"Subreddits: {df_reddit['subreddit'].value_counts().to_dict()}\")\nprint(f\"Date range: {df_reddit['date'].min()} to {df_reddit['date'].max()}\")\ndf_reddit.head()",
   "execution_count": null,
   "outputs": [],
   "id": "1ed45d64"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.2 WebMD Reviews Collection\n\nWe scraped patient reviews from **reviews.webmd.com** for Ozempic and Wegovy. The site embeds review data in a `window.__INITIAL_STATE__` JSON object on each page. We paginated through all available pages (20 reviews per page) using the `?page=N` parameter.\n\n**URLs:**\n- Ozempic: `https://reviews.webmd.com/drugs/drugreview-ozempic-semaglutide`\n- Wegovy: `https://reviews.webmd.com/drugs/drugreview-wegovy-semaglutide`",
   "id": "385c5e94"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# WebMD scraping approach - extract reviews from embedded JSON\nimport json\n\nurl = 'https://reviews.webmd.com/drugs/drugreview-ozempic-semaglutide?page=1'\nresponse = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'})\nsoup = BeautifulSoup(response.text, 'html.parser')\n\n# Extract reviews from __INITIAL_STATE__ embedded JSON\nfor script in soup.find_all('script'):\n    text = script.string or ''\n    if 'window.__INITIAL_STATE__' in text:\n        json_str = text.replace('window.__INITIAL_STATE__=', '').strip().rstrip(';')\n        data = json.loads(json_str)\n        sample_reviews = data['all_reviews']['drug_review_nimvs'][0]['review_nimvs']\n        print(f\"Sample page: {len(sample_reviews)} reviews extracted from page 1\")\n        print(f\"Sample review: {sample_reviews[0]['UserExperience'][:150]}...\")\n        break\n\n# Full collection was done via collect_webmd_real.py (paginated through all pages)\n# Loading pre-collected data:\ndf_webmd = pd.read_csv('../data/webmd_reviews.csv')\nprint(f\"\\nTotal WebMD reviews: {len(df_webmd)}\")\nprint(f\"By drug: {df_webmd['drug'].value_counts().to_dict()}\")\ndf_webmd.head()",
   "execution_count": null,
   "outputs": [],
   "id": "5e09f77b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.3 News Articles Collection\n\nWe collected news articles about GLP-1 drugs via **Google News RSS feeds**. Multiple search queries were used to capture diverse coverage (weight loss, side effects, insurance, shortages, clinical trials). Articles were deduplicated by title.",
   "id": "f48f5c8a"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# News collection via Google News RSS\nimport re\n\nqueries = [\n    'Ozempic weight loss', 'Wegovy weight loss', 'semaglutide obesity',\n    'GLP-1 weight loss drug', 'Ozempic side effects',\n    'Wegovy insurance coverage', 'Ozempic shortage', 'semaglutide clinical trial',\n]\n\n# Example: fetch one query\nsample_query = queries[0]\nrss_url = f\"https://news.google.com/rss/search?q={sample_query.replace(' ', '+')}+after:2024-01-01+before:2024-12-01&hl=en-US&gl=US&ceid=US:en\"\nresponse = requests.get(rss_url, headers={'User-Agent': 'Mozilla/5.0'})\nsoup = BeautifulSoup(response.text, 'xml')\nitems = soup.find_all('item')\nprint(f\"Sample RSS query '{sample_query}': {len(items)} articles\")\nif items:\n    title = items[0].find('title').get_text(strip=True)\n    source = items[0].find('source').get_text(strip=True) if items[0].find('source') else 'Unknown'\n    print(f\"  First article: [{source}] {title[:100]}\")\n\n# Full collection used 8 queries, deduplicated by title, cleaned HTML from descriptions\n# Loading pre-collected data:\ndf_news = pd.read_csv('../data/news_articles.csv')\nprint(f\"\\nTotal news articles: {len(df_news)}\")\nprint(f\"Unique sources: {df_news['source'].nunique()}\")\nprint(f\"Top 5 sources: {df_news['source'].value_counts().head(5).to_dict()}\")\ndf_news.head()",
   "execution_count": null,
   "outputs": [],
   "id": "022dddf4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Data Summary"
   ],
   "id": "eee9f7e3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 50)",
    "print(\"DATA COLLECTION SUMMARY\")",
    "print(\"=\" * 50)",
    "print(f\"\\nReddit posts:     {len(df_reddit):>6}\")",
    "print(f\"WebMD reviews:    {len(df_webmd):>6}\")",
    "print(f\"News articles:    {len(df_news):>6}\")",
    "print(f\"{'\u2500' * 30}\")",
    "print(f\"Total documents:  {len(df_reddit) + len(df_webmd) + len(df_news):>6}\")",
    "print(f\"\\nPublic corpus:    {len(df_reddit) + len(df_webmd):>6} (Reddit + WebMD)\")",
    "print(f\"Media corpus:     {len(df_news):>6} (News articles)\")",
    "",
    "print(f\"\\nDate range: {df_reddit['date'].min()} to {df_reddit['date'].max()}\")",
    "print(f\"\\nReddit subreddits: {df_reddit['subreddit'].unique().tolist()}\")",
    "print(f\"News sources: {df_news['source'].nunique()} unique outlets\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "bc7777c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}